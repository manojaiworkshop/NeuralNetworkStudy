#ifndef LSTM_CUDA_H
#define LSTM_CUDA_H

#include "matrix_cuda.h"
#include "activation_cuda.h"
#include <memory>
#include <vector>
#include <string>
#include <cuda_runtime.h>

/**
 * @file lstm_cuda.h
 * @brief GPU-accelerated Long Short-Term Memory (LSTM) implementation
 * 
 * CUDA-accelerated LSTM for high-performance sequence processing
 * 
 * KEY OPTIMIZATIONS:
 * - Fused gate computations on GPU
 * - Parallel element-wise operations
 * - Optimized memory access patterns
 * - Stream processing for overlapping computation
 * 
 * PERFORMANCE BENEFITS:
 * - 20-100x speedup on large sequences
 * - Efficient for batch_size >= 32
 * - Best for hidden_size >= 256
 */

/**
 * @brief CUDA-accelerated LSTM Cell
 */
class LSTMCellCUDA {
private:
    size_t input_size;
    size_t hidden_size;
    
    // Device (GPU) parameters - 4 gates Ã— 2 weight matrices each
    float* d_W_f;  // Forget gate: input weights
    float* d_U_f;  // Forget gate: hidden weights
    float* d_b_f;  // Forget gate: bias
    
    float* d_W_i;  // Input gate: input weights
    float* d_U_i;  // Input gate: hidden weights
    float* d_b_i;  // Input gate: bias
    
    float* d_W_c;  // Cell candidate: input weights
    float* d_U_c;  // Cell candidate: hidden weights
    float* d_b_c;  // Cell candidate: bias
    
    float* d_W_o;  // Output gate: input weights
    float* d_U_o;  // Output gate: hidden weights
    float* d_b_o;  // Output gate: bias
    
    // Host (CPU) copies for weight updates
    MatrixCUDA W_f, U_f, b_f;
    MatrixCUDA W_i, U_i, b_i;
    MatrixCUDA W_c, U_c, b_c;
    MatrixCUDA W_o, U_o, b_o;
    
    // Gradients on GPU
    float* d_dW_f; float* d_dU_f; float* d_db_f;
    float* d_dW_i; float* d_dU_i; float* d_db_i;
    float* d_dW_c; float* d_dU_c; float* d_db_c;
    float* d_dW_o; float* d_dU_o; float* d_db_o;
    
    // Activations
    std::unique_ptr<ActivationCUDA> sigmoid;
    std::unique_ptr<ActivationCUDA> tanh_activation;
    
    // Cached data for backward pass
    MatrixCUDA cached_input;
    MatrixCUDA cached_prev_hidden;
    MatrixCUDA cached_prev_cell;
    MatrixCUDA cached_forget_gate;
    MatrixCUDA cached_input_gate;
    MatrixCUDA cached_candidate;
    MatrixCUDA cached_output_gate;
    MatrixCUDA cached_cell;
    MatrixCUDA cached_hidden;
    
    // Memory management
    void allocateGPU();
    void freeGPU();
    void copyWeightsToGPU();
    void copyWeightsFromGPU();
    
public:
    /**
     * @brief Constructor
     */
    LSTMCellCUDA(size_t input_size, size_t hidden_size);
    
    /**
     * @brief Destructor
     */
    ~LSTMCellCUDA();
    
    /**
     * @brief Initialize weights
     */
    void initializeWeights(const std::string& strategy = "xavier");
    
    /**
     * @brief Forward pass on GPU
     * @param input Input at current time step
     * @param prev_hidden Previous hidden state
     * @param prev_cell Previous cell state
     * @return Pair of (new_hidden, new_cell)
     */
    std::pair<MatrixCUDA, MatrixCUDA> forward(const MatrixCUDA& input,
                                               const MatrixCUDA& prev_hidden,
                                               const MatrixCUDA& prev_cell);
    
    /**
     * @brief Backward pass on GPU
     */
    std::tuple<MatrixCUDA, MatrixCUDA, MatrixCUDA> backward(
        const MatrixCUDA& grad_hidden,
        const MatrixCUDA& grad_cell);
    
    /**
     * @brief Update parameters on GPU
     */
    void updateParameters(double learning_rate);
    
    /**
     * @brief Reset gradients
     */
    void resetGradients();
    
    // Getters
    size_t getInputSize() const { return input_size; }
    size_t getHiddenSize() const { return hidden_size; }
    int getParameterCount() const;
};

/**
 * @brief CUDA-accelerated LSTM Layer
 */
class LSTMLayerCUDA {
private:
    size_t input_size;
    size_t hidden_size;
    size_t output_size;
    bool return_sequences;
    
    LSTMCellCUDA cell;
    
    // Output layer on GPU
    MatrixCUDA W_hy;
    MatrixCUDA b_y;
    MatrixCUDA dW_hy;
    MatrixCUDA db_y;
    
    // Cached sequences
    std::vector<MatrixCUDA> hidden_states;
    std::vector<MatrixCUDA> cell_states;
    std::vector<MatrixCUDA> inputs;
    
    std::unique_ptr<ActivationCUDA> output_activation;
    
public:
    /**
     * @brief Constructor
     */
    LSTMLayerCUDA(size_t input_size, size_t hidden_size, size_t output_size,
                  bool return_sequences = false,
                  ActivationCUDA* output_activation = nullptr);
    
    /**
     * @brief Forward pass through sequence on GPU
     */
    MatrixCUDA forward(const std::vector<MatrixCUDA>& sequence,
                       const MatrixCUDA& initial_hidden = MatrixCUDA(),
                       const MatrixCUDA& initial_cell = MatrixCUDA());
    
    /**
     * @brief Backward pass through sequence on GPU
     */
    std::vector<MatrixCUDA> backward(const MatrixCUDA& output_gradient);
    
    /**
     * @brief Update parameters
     */
    void updateParameters(double learning_rate);
    
    /**
     * @brief Reset gradients
     */
    void resetGradients();
    
    /**
     * @brief Initialize weights
     */
    void initializeWeights(const std::string& strategy = "xavier");
    
    // Getters
    size_t getInputSize() const { return input_size; }
    size_t getHiddenSize() const { return hidden_size; }
    size_t getOutputSize() const { return output_size; }
    int getParameterCount() const;
};

#endif // LSTM_CUDA_H
