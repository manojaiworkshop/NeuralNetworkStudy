/**
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * RNN, LSTM, GRU: SIDE-BY-SIDE COMPARISON
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * 
 * This example demonstrates the differences between:
 * - Vanilla RNN (simple, fast, but vanishing gradients)
 * - LSTM (complex, powerful, remembers long-term)
 * - GRU (balanced, fewer parameters than LSTM)
 * 
 * We'll use a simple task: Predict sine wave
 */

#include "../include/nn/rnn.h"
#include "../include/nn/lstm.h"
#include "../include/nn/gru.h"
#include "../include/nn/matrix.h"
#include <iostream>
#include <iomanip>
#include <cmath>
#include <vector>
#include <chrono>

// ANSI Colors
#define RESET   "\033[0m"
#define GREEN   "\033[32m"
#define YELLOW  "\033[33m"
#define BLUE    "\033[34m"
#define CYAN    "\033[36m"
#define BOLD    "\033[1m"

void printHeader(const std::string& title) {
    std::cout << "\n" << BOLD << CYAN;
    std::cout << "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n";
    std::cout << "â•‘  " << std::setw(56) << std::left << title << "  â•‘\n";
    std::cout << "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•";
    std::cout << RESET << "\n\n";
}

// Generate simple sine wave sequence
std::vector<Matrix> generateSequence(int length, double start = 0.0) {
    std::vector<Matrix> sequence;
    for (int i = 0; i < length; ++i) {
        Matrix input(1, 1);
        input.set(0, 0, std::sin(start + i * 0.1));
        sequence.push_back(input);
    }
    return sequence;
}

int main() {
    std::cout << BOLD << CYAN << R"(
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                               â•‘
â•‘       RNN vs LSTM vs GRU: Architectural Comparison           â•‘
â•‘                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    )" << RESET << "\n";

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // PART 1: Architecture Comparison
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    printHeader("PART 1: Architecture Comparison");
    
    std::cout << BOLD << "1. VANILLA RNN:\n" << RESET;
    std::cout << "   Equations:\n";
    std::cout << "     h(t) = tanh(W_xhÂ·x(t) + W_hhÂ·h(t-1) + b_h)\n";
    std::cout << "     y(t) = W_hyÂ·h(t) + b_y\n\n";
    std::cout << "   Parameters: 2 weight matrices\n";
    std::cout << "   â€¢ W_xh: input â†’ hidden\n";
    std::cout << "   â€¢ W_hh: hidden â†’ hidden (recurrence)\n\n";
    std::cout << "   Pros: âœ“ Simple, fast\n";
    std::cout << "   Cons: âœ— Vanishing gradients on long sequences\n\n";
    
    std::cout << BOLD << "2. LSTM (Long Short-Term Memory):\n" << RESET;
    std::cout << "   Components: 4 gates + cell state\n\n";
    std::cout << "   Forget Gate:  f(t) = Ïƒ(W_fÂ·[h,x] + b_f)\n";
    std::cout << "     â””â”€ Decides what to remove from memory\n\n";
    std::cout << "   Input Gate:   i(t) = Ïƒ(W_iÂ·[h,x] + b_i)\n";
    std::cout << "                 CÌƒ(t) = tanh(W_cÂ·[h,x] + b_c)\n";
    std::cout << "     â””â”€ Decides what to add to memory\n\n";
    std::cout << "   Cell Update:  C(t) = f(t)âŠ™C(t-1) + i(t)âŠ™CÌƒ(t)\n";
    std::cout << "     â””â”€ The MEMORY HIGHWAY (addition preserves gradients!)\n\n";
    std::cout << "   Output Gate:  o(t) = Ïƒ(W_oÂ·[h,x] + b_o)\n";
    std::cout << "                 h(t) = o(t)âŠ™tanh(C(t))\n";
    std::cout << "     â””â”€ Decides what to output\n\n";
    std::cout << "   Parameters: 8 weight matrices (4 gates Ã— 2 each)\n";
    std::cout << "   Pros: âœ“ Solves vanishing gradients\n";
    std::cout << "         âœ“ Remembers long-term dependencies\n";
    std::cout << "   Cons: âœ— More parameters\n";
    std::cout << "         âœ— Slower training\n\n";
    
    std::cout << BOLD << "3. GRU (Gated Recurrent Unit):\n" << RESET;
    std::cout << "   Components: 3 gates (simpler than LSTM)\n\n";
    std::cout << "   Update Gate:  z(t) = Ïƒ(W_zÂ·[h,x] + b_z)\n";
    std::cout << "     â””â”€ How much to update (combines input/forget)\n\n";
    std::cout << "   Reset Gate:   r(t) = Ïƒ(W_rÂ·[h,x] + b_r)\n";
    std::cout << "     â””â”€ How much past to forget\n\n";
    std::cout << "   Candidate:    hÌƒ(t) = tanh(W_hÂ·[râŠ™h,x] + b_h)\n";
    std::cout << "   Final Hidden: h(t) = z(t)âŠ™h(t-1) + (1-z(t))âŠ™hÌƒ(t)\n";
    std::cout << "     â””â”€ Interpolate between old and new\n\n";
    std::cout << "   Parameters: 6 weight matrices (3 gates Ã— 2 each)\n";
    std::cout << "   Pros: âœ“ Fewer parameters than LSTM (faster)\n";
    std::cout << "         âœ“ Still handles long-term dependencies\n";
    std::cout << "         âœ“ Often performs similarly to LSTM\n";
    std::cout << "   Cons: âœ— Less flexible than LSTM for some tasks\n\n";
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // PART 2: Parameter Count Comparison
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    printHeader("PART 2: Parameter Count Comparison");
    
    int input_size = 1;
    int hidden_size = 8;
    int output_size = 1;
    
    // Create cells
    RNNCell rnn_cell(input_size, hidden_size);
    LSTMCell lstm_cell(input_size, hidden_size);
    GRUCell gru_cell(input_size, hidden_size);
    
    std::cout << "Configuration: input=" << input_size 
              << ", hidden=" << hidden_size << "\n\n";
    
    std::cout << std::left;
    std::cout << "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n";
    std::cout << "â”‚ Model      â”‚ Parameters  â”‚ Memory Footprint         â”‚\n";
    std::cout << "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n";
    std::cout << "â”‚ RNN        â”‚ " << std::setw(11) << rnn_cell.getParameterCount() 
              << " â”‚ " << std::setw(24) << "Small (fast training)" << " â”‚\n";
    std::cout << "â”‚ LSTM       â”‚ " << std::setw(11) << lstm_cell.getParameterCount() 
              << " â”‚ " << std::setw(24) << "Large (4Ã— RNN)" << " â”‚\n";
    std::cout << "â”‚ GRU        â”‚ " << std::setw(11) << gru_cell.getParameterCount() 
              << " â”‚ " << std::setw(24) << "Medium (3Ã— RNN)" << " â”‚\n";
    std::cout << "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n";
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // PART 3: When to Use Each?
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    printHeader("PART 3: When to Use Each Architecture?");
    
    std::cout << BOLD << "ğŸ¯ USE RNN WHEN:\n" << RESET;
    std::cout << "   â€¢ Short sequences (< 10 steps)\n";
    std::cout << "   â€¢ Speed is critical\n";
    std::cout << "   â€¢ Simple patterns\n";
    std::cout << "   Example: Real-time sensor data (last few readings)\n\n";
    
    std::cout << BOLD << "ğŸ¯ USE LSTM WHEN:\n" << RESET;
    std::cout << "   â€¢ Long sequences (20-100+ steps)\n";
    std::cout << "   â€¢ Complex long-term dependencies\n";
    std::cout << "   â€¢ You have enough data/compute\n";
    std::cout << "   Examples:\n";
    std::cout << "     â€¢ Language modeling (sentences, paragraphs)\n";
    std::cout << "     â€¢ Video analysis (many frames)\n";
    std::cout << "     â€¢ Time series with trends\n\n";
    
    std::cout << BOLD << "ğŸ¯ USE GRU WHEN:\n" << RESET;
    std::cout << "   â€¢ Medium sequences (10-50 steps)\n";
    std::cout << "   â€¢ Want LSTM performance with fewer parameters\n";
    std::cout << "   â€¢ Limited training data\n";
    std::cout << "   â€¢ Faster training needed\n";
    std::cout << "   Examples:\n";
    std::cout << "     â€¢ Speech recognition\n";
    std::cout << "     â€¢ Machine translation\n";
    std::cout << "     â€¢ Music generation\n\n";
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // PART 4: Quick Demonstration
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    printHeader("PART 4: Forward Pass Demonstration");
    
    std::cout << "Processing short sequence through all three models...\n\n";
    
    // Create layers
    RNNLayer rnn_layer(input_size, hidden_size, output_size, false);
    LSTMLayer lstm_layer(input_size, hidden_size, output_size, false);
    GRULayer gru_layer(input_size, hidden_size, output_size, false);
    
    // Generate test sequence
    auto sequence = generateSequence(5, 0.0);
    
    std::cout << "Input sequence (sine wave):\n  ";
    for (size_t i = 0; i < sequence.size(); ++i) {
        std::cout << std::fixed << std::setprecision(3) << sequence[i].get(0, 0);
        if (i < sequence.size() - 1) std::cout << " â†’ ";
    }
    std::cout << "\n\n";
    
    // Forward passes
    auto start = std::chrono::high_resolution_clock::now();
    Matrix rnn_out = rnn_layer.forward(sequence);
    auto rnn_time = std::chrono::duration_cast<std::chrono::microseconds>(
        std::chrono::high_resolution_clock::now() - start).count();
    
    start = std::chrono::high_resolution_clock::now();
    Matrix lstm_out = lstm_layer.forward(sequence);
    auto lstm_time = std::chrono::duration_cast<std::chrono::microseconds>(
        std::chrono::high_resolution_clock::now() - start).count();
    
    start = std::chrono::high_resolution_clock::now();
    Matrix gru_out = gru_layer.forward(sequence);
    auto gru_time = std::chrono::duration_cast<std::chrono::microseconds>(
        std::chrono::high_resolution_clock::now() - start).count();
    
    std::cout << "Outputs (untrained networks, random initialization):\n";
    std::cout << "  RNN:  " << std::fixed << std::setprecision(6) 
              << rnn_out.get(0, 0) << " (" << rnn_time << " Î¼s)\n";
    std::cout << "  LSTM: " << lstm_out.get(0, 0) << " (" << lstm_time << " Î¼s)\n";
    std::cout << "  GRU:  " << gru_out.get(0, 0) << " (" << gru_time << " Î¼s)\n\n";
    
    std::cout << "Relative speed:\n";
    double base = rnn_time;
    std::cout << "  RNN:  1.00x (baseline)\n";
    std::cout << "  LSTM: " << std::fixed << std::setprecision(2) 
              << (double)lstm_time/base << "x slower\n";
    std::cout << "  GRU:  " << (double)gru_time/base << "x slower\n\n";
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // SUMMARY
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    printHeader("SUMMARY: Key Differences");
    
    std::cout << "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n";
    std::cout << "â”‚ Feature     â”‚ RNN      â”‚ LSTM      â”‚ GRU              â”‚\n";
    std::cout << "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n";
    std::cout << "â”‚ Complexity  â”‚ Simple   â”‚ Complex   â”‚ Moderate         â”‚\n";
    std::cout << "â”‚ Parameters  â”‚ Fewest   â”‚ Most      â”‚ Middle           â”‚\n";
    std::cout << "â”‚ Speed       â”‚ Fastest  â”‚ Slowest   â”‚ Fast             â”‚\n";
    std::cout << "â”‚ Long Memory â”‚ Poor     â”‚ Excellent â”‚ Very Good        â”‚\n";
    std::cout << "â”‚ Training    â”‚ Easy     â”‚ Hard      â”‚ Moderate         â”‚\n";
    std::cout << "â”‚ Use Case    â”‚ Short    â”‚ Long      â”‚ General Purpose  â”‚\n";
    std::cout << "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n";
    
    std::cout << BOLD << GREEN << "âœ“ Example completed!\n" << RESET;
    std::cout << "\nKey Takeaway:\n";
    std::cout << "  â€¢ Start with GRU (best balance)\n";
    std::cout << "  â€¢ Use LSTM if GRU doesn't work\n";
    std::cout << "  â€¢ Use RNN only for very short sequences\n\n";
    
    std::cout << YELLOW << "For GPU acceleration, check:\n" << RESET;
    std::cout << "  â€¢ lstm_cuda_example - LSTM on GPU\n";
    std::cout << "  â€¢ gru_cuda_example - GRU on GPU\n";
    std::cout << "  â€¢ Expect 20-100x speedup for large batches!\n\n";
    
    return 0;
}
