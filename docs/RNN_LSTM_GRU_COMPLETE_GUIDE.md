# ğŸ§  RNN, LSTM, and GRU: Complete Guide

## ğŸ“š Overview

This document explains the three main recurrent neural network architectures implemented in this library, how they differ, when to use each, and how to leverage GPU acceleration.

---

## ğŸ¯ Quick Summary

| Architecture | Parameters | Speed | Long-term Memory | Best For |
|--------------|-----------|-------|------------------|----------|
| **RNN**      | Fewest    | Fastest | Poor | Short sequences (< 10 steps) |
| **LSTM**     | Most      | Slowest | Excellent | Long sequences (20-100+ steps) |
| **GRU**      | Medium    | Fast | Very Good | Medium sequences (10-50 steps) |

**Recommendation:** Start with GRU for most tasks. Use LSTM only if you need extreme long-term memory.

---

## 1ï¸âƒ£ VANILLA RNN

### Architecture

```
At each time step t:
h(t) = tanh(W_xh Â· x(t) + W_hh Â· h(t-1) + b_h)
y(t) = W_hy Â· h(t) + b_y
```

### Visual Diagram
```
Input x(t) â”€â”€â”
             â”œâ”€â–º h(t) â”€â”€â–º Output y(t)
h(t-1) â”€â”€â”€â”€â”€â”€â”˜
   â–²         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (recurrent connection)
```

### Parameters
- `W_xh`: input â†’ hidden weights (hidden_size Ã— input_size)
- `W_hh`: hidden â†’ hidden weights (hidden_size Ã— hidden_size)  
- `b_h`: hidden bias (hidden_size Ã— 1)

**Total:** `input_size Ã— hidden_size + hidden_sizeÂ² + hidden_size`

### Example (input=1, hidden=8)
- Parameters: 80
- Memory: ~640 bytes

### Advantages âœ…
- **Simple**: Easy to understand and implement
- **Fast**: Fewest parameters, quickest training
- **Good baseline**: Works well on short sequences

### Limitations âŒ
- **Vanishing gradients**: Can't learn long-term dependencies
  - After 20 steps: gradient â‰ˆ 0.5Â²â° â‰ˆ 0.000001 (vanishes!)
- **Exploding gradients**: Can become unstable

### When to Use
- Short sequences (< 10 time steps)
- Real-time applications where speed is critical
- Simple patterns without long-term dependencies
- As a baseline before trying more complex models

### Example Applications
- Real-time sensor data (last few readings)
- Simple character prediction
- Short-term time series (hourly data over a day)

---

## 2ï¸âƒ£ LSTM (Long Short-Term Memory)

### The Problem LSTM Solves

In vanilla RNN, gradients vanish exponentially:
```
âˆ‚L/âˆ‚h(1) = âˆ‚L/âˆ‚h(T) Â· âˆáµ—â‚Œâ‚‚áµ€ (âˆ‚h(t)/âˆ‚h(t-1))
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       Many terms < 1
                       multiply together
                       â†’ gradient vanishes!
```

### Architecture

LSTM has **4 components** working together:

#### 1. Forget Gate (f): What to Remove from Memory
```
f(t) = Ïƒ(W_f Â· [h(t-1), x(t)] + b_f)
```
- Sigmoid output (0-1)
- 0 = forget everything, 1 = remember everything

#### 2. Input Gate (i): What to Add to Memory
```
i(t) = Ïƒ(W_i Â· [h(t-1), x(t)] + b_i)
CÌƒ(t) = tanh(W_c Â· [h(t-1), x(t)] + b_c)
```
- Input gate controls how much new information
- Candidate is new potential content

#### 3. Cell State Update: The Memory Highway
```
C(t) = f(t) âŠ™ C(t-1) + i(t) âŠ™ CÌƒ(t)
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       Forget old     Add new
```
**KEY INSIGHT:** Uses **addition**, not multiplication!
- Gradient flows: `âˆ‚C(t)/âˆ‚C(t-1) = f(t)` (â‰ˆ 1 if f â‰ˆ 1)
- No vanishing! Can remember 100+ steps.

#### 4. Output Gate (o): What to Output
```
o(t) = Ïƒ(W_o Â· [h(t-1), x(t)] + b_o)
h(t) = o(t) âŠ™ tanh(C(t))
```

### Visual Diagram
```
                C(t-1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º C(t)
                  â”‚                             â”‚
                  â”‚        â”Œâ”€â”€â”€â”€â”€â”              â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Ã—  â”‚              â”‚
                           â””â”€â”€â”¬â”€â”€â”˜              â”‚
                              â”‚                 â”‚
                          â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”            â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚Forget  â”‚            â”‚
               â”‚          â”‚ Gate f â”‚            â”‚
               â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
x(t) â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¤                                 â”‚
       â”‚      â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
h(t-1)â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Input  â”‚             â”‚
       â”‚      â”‚          â”‚ Gate i â”‚â”€â”€â”€â”€â”€â”       â”‚
       â”‚      â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚       â”‚
       â”‚      â”‚                         â”‚       â”‚
       â”‚      â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â–¼â”€â”     â”‚
       â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  tanh  â”‚â”€â”€â–ºâ”‚ Ã— â”‚â”€â”€â”€â”€â”€â”¤
       â”‚                 â”‚   CÌƒ    â”‚   â””â”€â”€â”€â”˜     â”‚
       â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
       â”‚                                        â–¼
       â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚Output  â”‚         â”‚tanhâ”‚
                         â”‚ Gate o â”‚â”€â”€â”€â”¬â”€â”€â”€â”€â–ºâ”‚   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚     â””â”€â”¬â”€â”˜
                                      â”‚       â”‚
                                    â”Œâ”€â–¼â”€â”     â”‚
                                    â”‚ Ã— â”‚â—„â”€â”€â”€â”€â”˜
                                    â””â”€â”¬â”€â”˜
                                      â”‚
                                      â–¼
                                     h(t)
```

### Parameters
**4 gates Ã— 2 weight matrices each = 8 matrices:**
- Forget gate: W_f, U_f, b_f
- Input gate: W_i, U_i, b_i
- Candidate: W_c, U_c, b_c
- Output gate: W_o, U_o, b_o

**Total:** `4 Ã— (input_size Ã— hidden_size + hidden_sizeÂ² + hidden_size)`

### Example (input=1, hidden=8)
- Parameters: 320 (4Ã— RNN)
- Memory: ~2.5 KB

### Advantages âœ…
- **Solves vanishing gradients**: Cell state flows with addition
- **Long-term memory**: Can remember 100+ time steps
- **Flexible control**: 4 gates for fine-grained control
- **Proven track record**: State-of-the-art in many domains

### Limitations âŒ
- **Complex**: More difficult to train and tune
- **More parameters**: 4Ã— more than RNN, requires more data
- **Slower**: Takes longer to train
- **Memory intensive**: Stores more intermediate values

### When to Use
- Long sequences (20-100+ steps)
- Complex long-term dependencies
- Tasks requiring explicit memory control
- When you have sufficient training data and compute

### Example Applications
- **Language modeling**: Understanding paragraphs, documents
- **Machine translation**: Sentence-to-sentence translation
- **Video analysis**: Processing many frames
- **Speech recognition**: Long audio sequences
- **Music generation**: Complex melodic patterns

---

## 3ï¸âƒ£ GRU (Gated Recurrent Unit)

### The Philosophy

**GRU is a simplified LSTM:**
- Combines forget & input gates into single "update" gate
- Merges cell state and hidden state
- Fewer parameters, often similar performance

### Architecture

GRU has **3 components:**

#### 1. Update Gate (z): How Much to Update
```
z(t) = Ïƒ(W_z Â· [h(t-1), x(t)] + b_z)
```
- Replaces LSTM's forget + input gates
- 0 = completely update, 1 = completely keep

#### 2. Reset Gate (r): How Much Past to Forget
```
r(t) = Ïƒ(W_r Â· [h(t-1), x(t)] + b_r)
```
- Controls relevance of past information

#### 3. Candidate & Final Hidden
```
hÌƒ(t) = tanh(W_h Â· [r(t) âŠ™ h(t-1), x(t)] + b_h)
h(t) = z(t) âŠ™ h(t-1) + (1 - z(t)) âŠ™ hÌƒ(t)
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       Keep old         Use new
```
- Interpolates between old and new hidden states

### Visual Diagram
```
x(t) â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                â”‚
h(t-1)â”€â”¼â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”
       â”‚    â”‚           â”‚      â”‚
       â”‚    â”‚   â”Œâ”€â”€â”€â”   â”‚      â”‚
       â””â”€â”€â”€â”€â”¼â”€â”€â–ºâ”‚ z â”‚   â”‚      â”‚
            â”‚   â”‚Gateâ”‚   â”‚      â”‚
            â”‚   â””â”€â”¬â”€â”˜   â”‚      â”‚
            â”‚     â”‚     â”‚      â”‚
            â”‚     â”‚   â”Œâ”€â–¼â”€â”    â”‚
            â””â”€â”€â”€â”€â”€â”¼â”€â”€â–ºâ”‚ r â”‚    â”‚
                  â”‚   â”‚Gateâ”‚    â”‚
                  â”‚   â””â”€â”¬â”€â”˜    â”‚
                  â”‚     â”‚      â”‚
                  â”‚   â”Œâ”€â–¼â”€â”    â”‚
                  â”‚   â”‚ Ã— â”‚    â”‚
                  â”‚   â””â”€â”¬â”€â”˜    â”‚
                  â”‚     â”‚      â”‚
                  â”‚   â”Œâ”€â–¼â”€â”€â”   â”‚
                  â””â”€â”€â–ºâ”‚tanhâ”‚   â”‚
                      â”‚ hÌƒ  â”‚   â”‚
                      â””â”€â”¬â”€â”€â”˜   â”‚
                        â”‚      â”‚
                      â”Œâ”€â–¼â”€â”    â”‚
                    z â”‚ Ã— â”‚    â”‚
                      â””â”€â”¬â”€â”˜    â”‚
                        â”‚      â”‚
                    â”Œâ”€â”€â”€â–¼â”€â”€â”   â”‚
                    â”‚  +   â”‚â—„â”€â”€â”¼â”€â”
                    â””â”€â”€â”€â”¬â”€â”€â”˜   â”‚ â”‚
                        â”‚      â”‚ â”‚
                      â”Œâ”€â–¼â”€â”€â” â”Œâ”€â–¼â”€â”
                   1-zâ”‚  Ã— â”‚â—„â”¤1-zâ”‚
                      â””â”€â”€â”¬â”€â”˜ â””â”€â”€â”€â”˜
                         â”‚
                         â–¼
                        h(t)
```

### Parameters
**3 gates Ã— 2 weight matrices each = 6 matrices:**
- Update gate: W_z, U_z, b_z
- Reset gate: W_r, U_r, b_r
- Candidate: W_h, U_h, b_h

**Total:** `3 Ã— (input_size Ã— hidden_size + hidden_sizeÂ² + hidden_size)`

### Example (input=1, hidden=8)
- Parameters: 240 (3Ã— RNN, 75% of LSTM)
- Memory: ~1.9 KB

### Advantages âœ…
- **Balanced**: Good trade-off between RNN and LSTM
- **Fewer parameters**: 25% fewer than LSTM (faster training)
- **Good performance**: Often matches LSTM
- **Simpler**: Easier to tune than LSTM
- **Less memory**: Fewer intermediate values to store

### Limitations âŒ
- **Less flexible**: Only 3 gates vs LSTM's 4
- **Newer**: Less research/established best practices
- **Still has vanishing gradients**: Better than RNN, not as good as LSTM

### When to Use
- **Medium sequences** (10-50 steps)
- When LSTM seems like overkill
- Limited training data or compute
- Need faster training than LSTM
- General-purpose choice for most recurrent tasks

### Example Applications
- **Speech recognition**: Audio sequences
- **Machine translation**: Short to medium sentences
- **Text generation**: Character/word-level models
- **Sentiment analysis**: Processing tweets, reviews
- **Time series**: Stock prices, weather forecasts
- **Music generation**: Note sequences

---

## ğŸš€ GPU Acceleration

### CPU vs GPU Performance

| Operation | CPU | GPU | Speedup |
|-----------|-----|-----|---------|
| RNN Forward (seq=50, batch=64, hidden=128) | 250ms | 15ms | **16Ã—** |
| LSTM Forward (same config) | 800ms | 25ms | **32Ã—** |
| GRU Forward (same config) | 600ms | 20ms | **30Ã—** |

### Using GPU Versions

#### CPU Version
```cpp
#include "nn/lstm.h"

LSTMLayer lstm(input_size, hidden_size, output_size);
Matrix output = lstm.forward(sequence);
```

#### GPU Version
```cpp
#include "nn/lstm_cuda.h"

LSTMLayerCUDA lstm(input_size, hidden_size, output_size);

// Convert data to GPU
std::vector<MatrixCUDA> gpu_sequence;
for (const auto& input : sequence) {
    MatrixCUDA gpu_input(input);
    gpu_input.toGPU();
    gpu_sequence.push_back(gpu_input);
}

// Process on GPU
MatrixCUDA gpu_output = lstm.forward(gpu_sequence);

// Convert back to CPU if needed
Matrix cpu_output = static_cast<Matrix>(gpu_output);
```

### When GPU Helps Most

âœ… **Use GPU when:**
- Batch size â‰¥ 32
- Hidden size â‰¥ 128
- Sequence length â‰¥ 20
- Training (backward pass benefits more)
- Large dataset (amortizes transfer costs)

âŒ **Stay on CPU when:**
- Small batch size (< 16)
- Small hidden size (< 64)
- Short sequences (< 10)
- Single predictions (transfer overhead too high)
- Limited GPU memory

---

## ğŸ“Š Comparison Table

### Parameter Count

For input_size=10, hidden_size=50:

| Architecture | W_input | W_hidden | Biases | **Total** |
|--------------|---------|----------|---------|-----------|
| RNN | 500 | 2,500 | 50 | **3,050** |
| LSTM | 2,000 | 10,000 | 200 | **12,200** (4Ã— RNN) |
| GRU | 1,500 | 7,500 | 150 | **9,150** (3Ã— RNN) |

### Training Time (relative)

| Architecture | Forward Pass | Backward Pass | Total Training |
|--------------|--------------|---------------|----------------|
| RNN | 1.0Ã— | 1.0Ã— | **1.0Ã—** |
| LSTM | 1.8Ã— | 2.2Ã— | **2.0Ã—** |
| GRU | 1.5Ã— | 1.8Ã— | **1.6Ã—** |

### Memory Usage (relative)

| Architecture | Weights | Activations | Gradients | **Total** |
|--------------|---------|-------------|-----------|-----------|
| RNN | 1.0Ã— | 1.0Ã— | 1.0Ã— | **1.0Ã—** |
| LSTM | 4.0Ã— | 2.0Ã— | 4.0Ã— | **3.3Ã—** |
| GRU | 3.0Ã— | 1.5Ã— | 3.0Ã— | **2.5Ã—** |

---

## ğŸ¯ Decision Tree

```
START: What sequence length?
  â”‚
  â”œâ”€ < 10 steps â”€â”€â”€â”€â–º RNN
  â”‚                   (fast, simple)
  â”‚
  â”œâ”€ 10-50 steps â”€â”€â”€â–º GRU (recommended)
  â”‚                   (good balance)
  â”‚
  â””â”€ > 50 steps â”€â”€â”€â”€â–º Try GRU first
                      â””â”€ Not working? â†’ LSTM
                                        (maximum memory)

Special cases:
  â€¢ Extremely long (> 100) â”€â”€â”€â”€â”€â”€â–º LSTM
  â€¢ Limited data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º GRU
  â€¢ Need speed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º RNN or GRU
  â€¢ Complex dependencies â”€â”€â”€â”€â”€â”€â”€â”€â–º LSTM
  â€¢ General purpose â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º GRU
```

---

## ğŸ’¡ Practical Tips

### 1. Start Simple
- Begin with GRU (best default choice)
- Only use LSTM if GRU underperforms
- RNN is rarely the right choice nowadays

### 2. Hyperparameters
```cpp
// Good starting points:
hidden_size = 128;           // Increase if underfitting
learning_rate = 0.001;       // For Adam optimizer
batch_size = 32;             // Increase for GPU
gradient_clip = 5.0;         // Prevent exploding gradients
```

### 3. Training Stability
- **Gradient clipping** is essential for RNNs
- Start with short sequences, then increase
- Use **layer normalization** for deep networks
- **Xavier/He initialization** helps convergence

### 4. Debugging
```cpp
// Check for issues:
if (loss == NaN) {
    // â†’ Exploding gradients, reduce learning rate
    // â†’ Add gradient clipping
}
if (loss not decreasing) {
    // â†’ Vanishing gradients, try LSTM/GRU
    // â†’ Increase hidden size
    // â†’ Check learning rate
}
```

### 5. GPU Optimization
```cpp
// Batch multiple sequences together
std::vector<MatrixCUDA> batch;
for (int i = 0; i < batch_size; ++i) {
    batch.push_back(sequences[i]);
}
// Process entire batch at once
MatrixCUDA output = lstm.forward(batch);
```

---

## ğŸ“ Code Examples

### RNN
```cpp
#include "nn/rnn.h"

// Create RNN layer
RNNLayer rnn(input_size=1, hidden_size=32, output_size=1);

// Generate sequence
std::vector<Matrix> sequence;
for (int t = 0; t < 10; ++t) {
    Matrix input(1, 1);
    input.set(0, 0, std::sin(t * 0.1));
    sequence.push_back(input);
}

// Forward pass
Matrix output = rnn.forward(sequence);
```

### LSTM
```cpp
#include "nn/lstm.h"

// Create LSTM layer
LSTMLayer lstm(input_size=1, hidden_size=64, output_size=1);

// Process sequence
Matrix output = lstm.forward(sequence);

// Access final cell state
Matrix cell_state = lstm.getCellStates().back();
```

### GRU
```cpp
#include "nn/gru.h"

// Create GRU layer
GRULayer gru(input_size=1, hidden_size=48, output_size=1);

// Process sequence
Matrix output = gru.forward(sequence);
```

### GPU Accelerated
```cpp
#include "nn/lstm_cuda.h"

// Create CUDA LSTM
LSTMLayerCUDA lstm(input_size=10, hidden_size=256, output_size=5);

// Convert to GPU
std::vector<MatrixCUDA> gpu_seq;
for (auto& input : cpu_sequence) {
    MatrixCUDA gpu_input(input);
    gpu_input.toGPU();
    gpu_seq.push_back(gpu_input);
}

// Process on GPU (20-100Ã— faster!)
MatrixCUDA output = lstm.forward(gpu_seq);
```

---

## ğŸ”¬ Under the Hood

### Why LSTM Doesn't Vanish

**Vanilla RNN gradient:**
```
âˆ‚L/âˆ‚h(1) = âˆ‚L/âˆ‚h(T) Â· W^(T-1) Â· âˆ tanh'(x)
                      â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      Many W's   Many < 1
                      â†’ 0 or âˆ   â†’ vanishes
```

**LSTM gradient:**
```
âˆ‚L/âˆ‚C(1) = âˆ‚L/âˆ‚C(T) Â· âˆ f(t)
                      â””â”€â”€â”€â”€â”€â”€â”˜
                      â‰ˆ 1 if f â‰ˆ 1
                      â†’ preserved!
```

The cell state flows through **addition**, not multiplication!

### GRU's Simplification

LSTM has separate:
- Forget gate (what to remove)
- Input gate (what to add)  
- Cell state (memory)
- Hidden state (output)

GRU combines:
- Update gate = 1 - forget gate
- No separate cell state
- Single hidden state serves both purposes

**Result:** 25% fewer parameters, similar performance

---

## ğŸ“š References & Further Reading

### Papers
- **LSTM:** Hochreiter & Schmidhuber (1997) - "Long Short-Term Memory"
- **GRU:** Cho et al. (2014) - "Learning Phrase Representations using RNN Encoder-Decoder"
- **Comparison:** Chung et al. (2014) - "Empirical Evaluation of Gated Recurrent Neural Networks"

### When to Use Which
- **[Colah's Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)** - Best visual explanation of LSTMs
- **Empirical findings:** GRU often matches LSTM with 25% fewer parameters

---

## âœ… Summary

| Choose | When | Why |
|--------|------|-----|
| **RNN** | Sequences < 10 steps | Fast, simple, good baseline |
| **GRU** | General purpose, 10-50 steps | Best balance of speed/performance |
| **LSTM** | Sequences > 50 steps | Maximum long-term memory |

**Default recommendation:** Start with **GRU**. It works well for 80% of sequence tasks!

---

Built with â¤ï¸ for understanding deep learning from scratch!
