# ğŸ§  Complete Network Class Explanation - Building Neural Networks

## Overview: What is the Network Class?

The `NeuralNetwork` class is a **container** that manages multiple layers, coordinates training, and handles predictions. Think of it as the **brain** that orchestrates all the individual neurons (layers).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NEURAL NETWORK CLASS                            â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Container holding:                                          â”‚ â”‚
â”‚  â”‚  â€¢ Multiple Layers (vector<Layer*>)                         â”‚ â”‚
â”‚  â”‚  â€¢ Loss Function (Loss*)                                    â”‚ â”‚
â”‚  â”‚  â€¢ Optimizer (Optimizer*)                                   â”‚ â”‚
â”‚  â”‚                                                             â”‚ â”‚
â”‚  â”‚  Provides methods:                                          â”‚ â”‚
â”‚  â”‚  â€¢ addLayer() - build network                              â”‚ â”‚
â”‚  â”‚  â€¢ train() - learn from data                               â”‚ â”‚
â”‚  â”‚  â€¢ predict() - make predictions                            â”‚ â”‚
â”‚  â”‚  â€¢ forward() - propagate data forward                      â”‚ â”‚
â”‚  â”‚  â€¢ backward() - propagate gradients backward               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Network.h - Header File Line-by-Line

### Class Members (Private Data)

```cpp
// ============================================================================
// LINES 16-20: Private member variables - What the network stores
// ============================================================================

private:
    std::vector<std::unique_ptr<Layer>> layers;
    // Vector = dynamic array that can grow/shrink
    // unique_ptr = smart pointer that manages memory automatically
    // Layer = pointer to any layer type (DenseLayer, ConvLayer, etc.)
    //
    // Example:
    //   layers[0] â†’ DenseLayer (784 â†’ 128)
    //   layers[1] â†’ DenseLayer (128 â†’ 64)
    //   layers[2] â†’ DenseLayer (64 â†’ 10)
    //
    // Why vector?
    //   - Can add layers dynamically: layers.push_back()
    //   - Access by index: layers[i]
    //   - Iterate: for (auto& layer : layers)
    
    std::unique_ptr<Loss> loss_function;
    // Pointer to loss function object
    // Examples: MSELoss, BinaryCrossEntropyLoss, etc.
    // Used to measure how wrong predictions are
    //
    // Usage:
    //   double loss = loss_function->calculate(pred, target);
    
    std::unique_ptr<Optimizer> optimizer;
    // Optional optimizer for advanced training
    // Examples: SGD with momentum, Adam, RMSprop
    // If not set, uses simple gradient descent
    
    bool use_optimizer;
    // Flag: true = use optimizer, false = simple gradient descent


// ============================================================================
// KEY CONCEPT: Smart Pointers
// ============================================================================
//
// unique_ptr<Layer> means:
// â€¢ Automatic memory management (no memory leaks!)
// â€¢ When network is destroyed, layers are automatically deleted
// â€¢ Transfer ownership: network takes ownership of layers
//
// Without unique_ptr (old C++ way):
//   Layer* layer = new DenseLayer(10, 5);
//   // ... use layer ...
//   delete layer;  // MUST remember to free memory!
//
// With unique_ptr (modern C++):
//   std::unique_ptr<Layer> layer = std::make_unique<DenseLayer>(10, 5);
//   // ... use layer ...
//   // Automatic deletion when layer goes out of scope!
```

### Public Methods - Building the Network

```cpp
// ============================================================================
// LINES 30-34: addLayer() - Building the network architecture
// ============================================================================

void addLayer(Layer* layer);
// Add a layer to the network
//
// Parameters:
//   layer: Pointer to layer object (network takes ownership)
//
// What it does:
//   1. Takes raw pointer as input
//   2. Wraps in unique_ptr for automatic memory management
//   3. Adds to layers vector
//
// Example usage:
//   NeuralNetwork network;
//   network.addLayer(new DenseLayer(784, 128, new ReLU()));
//   network.addLayer(new DenseLayer(128, 64, new ReLU()));
//   network.addLayer(new DenseLayer(64, 10, new Sigmoid()));
//
// After these calls:
//   layers[0] â†’ DenseLayer(784â†’128, ReLU)
//   layers[1] â†’ DenseLayer(128â†’64, ReLU)
//   layers[2] â†’ DenseLayer(64â†’10, Sigmoid)
//
// Network structure:
//   Input(784) â†’ [Layer0] â†’ (128) â†’ [Layer1] â†’ (64) â†’ [Layer2] â†’ Output(10)


// ============================================================================
// LINES 36-40: setLoss() - Defining how to measure error
// ============================================================================

void setLoss(Loss* loss);
// Set the loss function for training
//
// Parameters:
//   loss: Pointer to loss function object
//
// Example:
//   network.setLoss(new MSELoss());           // For regression
//   network.setLoss(new BinaryCrossEntropy()); // For binary classification
//   network.setLoss(new CategoricalCrossEntropy()); // For multi-class
//
// Loss function is used to:
//   1. Calculate error: loss = loss_function->calculate(pred, target)
//   2. Compute gradients: grad = loss_function->gradient(pred, target)


// ============================================================================
// LINES 42-46: setOptimizer() - Advanced training strategies
// ============================================================================

void setOptimizer(Optimizer* opt);
// Set optimizer for parameter updates
//
// Parameters:
//   opt: Pointer to optimizer object
//
// Example:
//   network.setOptimizer(new SGD(0.01, 0.9));  // SGD with momentum
//   network.setOptimizer(new Adam(0.001));     // Adam optimizer
//
// Without optimizer: Simple gradient descent
//   W = W - learning_rate Ã— gradient
//
// With optimizer: Smarter updates
//   - Momentum: Accumulates past gradients for smoother updates
//   - Adam: Adapts learning rate per parameter
//   - RMSprop: Divides gradient by running average
```

### Forward and Backward Propagation

```cpp
// ============================================================================
// LINES 48-53: forward() - Compute network output
// ============================================================================

Matrix forward(const Matrix& input);
// Propagate input through all layers to get output
//
// Parameters:
//   input: Input data matrix (batch_size Ã— input_features)
//
// Returns:
//   Output matrix (batch_size Ã— output_features)
//
// What it does internally:
//   Matrix output = input;
//   for each layer in layers:
//       output = layer->forward(output);
//   return output;
//
// Example with 3 layers:
//   Input X (4Ã—784) â†’ Layer0 â†’ H1 (4Ã—128) â†’ Layer1 â†’ H2 (4Ã—64) â†’ Layer2 â†’ Y (4Ã—10)
//
// Pseudocode:
//   forward(X):
//       H1 = layers[0]->forward(X)    // 784 â†’ 128
//       H2 = layers[1]->forward(H1)   // 128 â†’ 64
//       Y  = layers[2]->forward(H2)   // 64 â†’ 10
//       return Y


// ============================================================================
// LINES 55-59: backward() - Compute gradients for learning
// ============================================================================

void backward(const Matrix& loss_gradient);
// Backpropagate gradients through all layers
//
// Parameters:
//   loss_gradient: Gradient from loss function (âˆ‚L/âˆ‚output)
//
// What it does internally:
//   Matrix gradient = loss_gradient;
//   for layer in reversed(layers):
//       gradient = layer->backward(gradient);
//
// Example with 3 layers:
//   Loss gradient â†’ Layer2 â†’ grad2 â†’ Layer1 â†’ grad1 â†’ Layer0 â†’ grad0
//
// Pseudocode:
//   backward(loss_grad):
//       grad2 = layers[2]->backward(loss_grad)  // Backprop through output layer
//       grad1 = layers[1]->backward(grad2)      // Backprop through hidden layer 2
//       grad0 = layers[0]->backward(grad1)      // Backprop through hidden layer 1
//
// Each layer computes:
//   1. Gradient w.r.t. its weights: âˆ‚L/âˆ‚W
//   2. Gradient w.r.t. its biases: âˆ‚L/âˆ‚b
//   3. Gradient to pass back: âˆ‚L/âˆ‚input (returned)


// ============================================================================
// LINES 61-65: updateParameters() - Apply gradient descent
// ============================================================================

void updateParameters(double learning_rate = 0.01);
// Update all layer parameters using computed gradients
//
// Parameters:
//   learning_rate: Step size for gradient descent (default 0.01)
//
// Two modes:
//
// 1. Simple gradient descent (no optimizer):
//    for each layer:
//        W = W - learning_rate Ã— âˆ‚L/âˆ‚W
//        b = b - learning_rate Ã— âˆ‚L/âˆ‚b
//
// 2. With optimizer (SGD, Adam, etc.):
//    for each layer:
//        W_new = optimizer->update(W, âˆ‚L/âˆ‚W, "layer_i_weights")
//        b_new = optimizer->update(b, âˆ‚L/âˆ‚b, "layer_i_biases")
//
// Example:
//   Layer has W = [0.5, 0.3] and âˆ‚L/âˆ‚W = [0.02, -0.01]
//   With learning_rate = 0.1:
//     W_new = [0.5, 0.3] - 0.1 Ã— [0.02, -0.01]
//           = [0.5 - 0.002, 0.3 + 0.001]
//           = [0.498, 0.301]
```

### Training Methods

```cpp
// ============================================================================
// LINES 67-74: train() - Main training loop
// ============================================================================

void train(const Matrix& X_train, const Matrix& y_train, 
          int epochs, int batch_size = 32, 
          double learning_rate = 0.01, bool verbose = true);
// Train the network on dataset
//
// Parameters:
//   X_train: Training inputs (num_samples Ã— num_features)
//   y_train: Training targets (num_samples Ã— num_outputs)
//   epochs: Number of times to iterate through entire dataset
//   batch_size: Number of samples per mini-batch (default: 32)
//   learning_rate: Step size for updates (default: 0.01)
//   verbose: Print progress (default: true)
//
// What it does:
//   for epoch in 1..epochs:
//       1. Shuffle data
//       2. Split into mini-batches
//       3. For each batch:
//          a. Forward pass
//          b. Calculate loss
//          c. Backward pass
//          d. Update parameters
//       4. Print progress
//
// Example:
//   network.train(X_train, y_train, 
//                 epochs=100, 
//                 batch_size=32, 
//                 learning_rate=0.01);
//
// Output:
//   Epoch    1/100 - Loss: 0.523456
//   Epoch   10/100 - Loss: 0.234567
//   ...
//   Epoch  100/100 - Loss: 0.012345


// ============================================================================
// KEY CONCEPT: Mini-Batch Training
// ============================================================================
//
// Why mini-batches?
//
// 1. Full Batch (batch_size = all samples):
//    âœ“ Accurate gradient
//    âœ— Slow (must process all data before updating)
//    âœ— Large memory requirement
//
// 2. Stochastic (batch_size = 1):
//    âœ“ Fast updates
//    âœ— Noisy gradients
//    âœ— Unstable training
//
// 3. Mini-Batch (batch_size = 32, 64, 128, etc.):
//    âœ“ Good gradient estimate
//    âœ“ Faster than full batch
//    âœ“ More stable than stochastic
//    âœ“ Efficient GPU utilization
//
// Example with 1000 samples, batch_size=32:
//   Batch 1: samples 0-31
//   Batch 2: samples 32-63
//   ...
//   Batch 32: samples 992-999 (last 8 samples)
//
//   Total batches per epoch: 32
//   Parameters updated 32 times per epoch
```

### Prediction and Evaluation

```cpp
// ============================================================================
// LINES 95-99: predict() - Make predictions on new data
// ============================================================================

Matrix predict(const Matrix& input);
// Make predictions without training
//
// Parameters:
//   input: Input data (num_samples Ã— num_features)
//
// Returns:
//   Predictions (num_samples Ã— num_outputs)
//
// Example:
//   Matrix test_data(10, 784);  // 10 test images
//   Matrix predictions = network.predict(test_data);
//   // predictions: (10 Ã— 10) for 10-class classification
//
// Usage:
//   For binary classification:
//     if (predictions.get(0, 0) > 0.5) { /* class 1 */ } 
//     else { /* class 0 */ }
//
//   For multi-class:
//     int predicted_class = argmax(predictions.get(0, :));


// ============================================================================
// LINES 101-107: evaluate() - Test network performance
// ============================================================================

double evaluate(const Matrix& X_test, const Matrix& y_test);
// Calculate loss on test data
//
// Parameters:
//   X_test: Test inputs
//   y_test: Test targets
//
// Returns:
//   Loss value (double)
//
// Example:
//   double test_loss = network.evaluate(X_test, y_test);
//   std::cout << "Test Loss: " << test_loss << "\n";
//
// Used to check if network generalizes to unseen data


// ============================================================================
// LINES 109-116: accuracy() - Calculate classification accuracy
// ============================================================================

double accuracy(const Matrix& X, const Matrix& y);
// Calculate percentage of correct predictions
//
// Parameters:
//   X: Input data
//   y: True labels (one-hot encoded or class indices)
//
// Returns:
//   Accuracy percentage (0-100)
//
// What it does:
//   1. Make predictions
//   2. For each sample:
//      a. Find predicted class (argmax of predictions)
//      b. Find true class (argmax of targets)
//      c. If same, increment correct counter
//   3. Return (correct / total) Ã— 100
//
// Example:
//   Predictions: [[0.1, 0.8, 0.1], [0.7, 0.2, 0.1]]
//   Targets:     [[0, 1, 0],       [1, 0, 0]]
//   
//   Sample 1: argmax([0.1, 0.8, 0.1]) = 1, argmax([0,1,0]) = 1 â†’ correct âœ“
//   Sample 2: argmax([0.7, 0.2, 0.1]) = 0, argmax([1,0,0]) = 0 â†’ correct âœ“
//   
//   Accuracy = 2/2 Ã— 100 = 100%
```

---

## Network.cpp - Implementation Details

### Constructor and Setup

```cpp
// ============================================================================
// LINES 12-28: Constructor and basic setup methods
// ============================================================================

// Constructor
NeuralNetwork::NeuralNetwork() : use_optimizer(false) {}
// Creates empty network
// use_optimizer = false means simple gradient descent by default

// Add layer
void NeuralNetwork::addLayer(Layer* layer) {
    layers.push_back(std::unique_ptr<Layer>(layer));
}
// 1. Takes raw pointer
// 2. Wraps in unique_ptr (smart pointer)
// 3. Adds to layers vector
//
// Memory management:
//   - Caller creates: new DenseLayer(...)
//   - Network takes ownership: unique_ptr manages memory
//   - Network destructor automatically deletes all layers

// Set loss function
void NeuralNetwork::setLoss(Loss* loss) {
    loss_function = std::unique_ptr<Loss>(loss);
}
// Similar to addLayer - wraps in unique_ptr

// Set optimizer
void NeuralNetwork::setOptimizer(Optimizer* opt) {
    optimizer = std::unique_ptr<Optimizer>(opt);
    use_optimizer = true;  // Enable optimizer mode
}
```

### Forward Propagation Implementation

```cpp
// ============================================================================
// LINES 30-40: forward() - Data flows through layers
// ============================================================================

Matrix NeuralNetwork::forward(const Matrix& input) {
    // Check if network has layers
    if (layers.empty()) {
        throw std::runtime_error("Network has no layers");
    }
    // Safety check - can't forward pass with no layers!
    
    Matrix output = input;
    // Start with input matrix
    
    for (auto& layer : layers) {
        output = layer->forward(output);
    }
    // Pass through each layer sequentially
    // output becomes input to next layer
    
    return output;
    // Final output after all layers
}

// Example execution:
//   Input: X (4Ã—784)
//
//   Iteration 1:
//     output = X (4Ã—784)
//     output = layers[0]->forward(X)  â†’ (4Ã—128)
//
//   Iteration 2:
//     output = (4Ã—128)
//     output = layers[1]->forward(output)  â†’ (4Ã—64)
//
//   Iteration 3:
//     output = (4Ã—64)
//     output = layers[2]->forward(output)  â†’ (4Ã—10)
//
//   Return: output (4Ã—10)
```

### Backward Propagation Implementation

```cpp
// ============================================================================
// LINES 42-50: backward() - Gradients flow backward
// ============================================================================

void NeuralNetwork::backward(const Matrix& loss_gradient) {
    Matrix gradient = loss_gradient;
    // Start with loss gradient from loss function
    
    // Backpropagate through layers in REVERSE order
    for (int i = layers.size() - 1; i >= 0; --i) {
        gradient = layers[i]->backward(gradient);
    }
    // Each layer:
    //   1. Computes âˆ‚L/âˆ‚W and âˆ‚L/âˆ‚b (stores internally)
    //   2. Returns âˆ‚L/âˆ‚input (gradient for previous layer)
}

// Example execution with 3 layers:
//   loss_gradient = âˆ‚L/âˆ‚Å· (4Ã—10)
//
//   i = 2 (Layer 2 - Output):
//     gradient = (4Ã—10)
//     gradient = layers[2]->backward(gradient)  â†’ (4Ã—64)
//     Layer 2 stores: âˆ‚L/âˆ‚Wâ‚‚, âˆ‚L/âˆ‚bâ‚‚
//
//   i = 1 (Layer 1 - Hidden):
//     gradient = (4Ã—64)
//     gradient = layers[1]->backward(gradient)  â†’ (4Ã—128)
//     Layer 1 stores: âˆ‚L/âˆ‚Wâ‚, âˆ‚L/âˆ‚bâ‚
//
//   i = 0 (Layer 0 - Input):
//     gradient = (4Ã—128)
//     gradient = layers[0]->backward(gradient)  â†’ (4Ã—784)
//     Layer 0 stores: âˆ‚L/âˆ‚Wâ‚€, âˆ‚L/âˆ‚bâ‚€
//
// After backward(), all layers have gradients ready for update
```

### Parameter Updates

```cpp
// ============================================================================
// LINES 52-87: updateParameters() - Apply gradient descent
// ============================================================================

void NeuralNetwork::updateParameters(double learning_rate) {
    if (use_optimizer && optimizer) {
        // â”€â”€â”€ OPTIMIZER MODE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        for (size_t i = 0; i < layers.size(); ++i) {
            // Cast to DenseLayer to access weight methods
            DenseLayer* dense = dynamic_cast<DenseLayer*>(layers[i].get());
            
            if (dense) {
                // Update weights using optimizer
                std::string weight_id = "layer" + std::to_string(i) + "_weights";
                Matrix new_weights = optimizer->update(
                    dense->getWeights(),         // Current weights
                    dense->getWeightGradients(), // Gradients
                    weight_id                    // Unique ID for momentum/state
                );
                dense->setWeights(new_weights);
                
                // Update biases using optimizer
                std::string bias_id = "layer" + std::to_string(i) + "_biases";
                Matrix new_biases = optimizer->update(
                    dense->getBiases(),
                    dense->getBiasGradients(),
                    bias_id
                );
                dense->setBiases(new_biases);
                
                // Reset gradients for next iteration
                dense->resetGradients();
            }
        }
        
    } else {
        // â”€â”€â”€ SIMPLE GRADIENT DESCENT MODE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        for (auto& layer : layers) {
            layer->updateParameters(learning_rate);
            // Each layer does: W = W - lr Ã— âˆ‚L/âˆ‚W
        }
    }
}

// Why two modes?
//
// Simple Gradient Descent:
//   âœ“ Easy to understand
//   âœ“ Works for simple problems
//   âœ— Can be slow to converge
//   âœ— Sensitive to learning rate
//
// With Optimizer (Adam, SGD+Momentum):
//   âœ“ Faster convergence
//   âœ“ Adaptive learning rates
//   âœ“ Handles noisy gradients better
//   âœ— More complex
//   âœ— Requires tuning hyperparameters
```

### Training Loop Implementation

```cpp
// ============================================================================
// LINES 154-204: train() - Complete training procedure
// ============================================================================

void NeuralNetwork::train(const Matrix& X_train, const Matrix& y_train, 
                         int epochs, int batch_size, 
                         double learning_rate, bool verbose) {
    
    // Check loss function is set
    if (!loss_function) {
        throw std::runtime_error("Loss function not set");
    }
    
    // â”€â”€â”€ EPOCH LOOP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for (int epoch = 0; epoch < epochs; ++epoch) {
        
        // Step 1: Shuffle data
        Matrix X_shuffled = X_train;
        Matrix y_shuffled = y_train;
        shuffleData(X_shuffled, y_shuffled);
        // Why shuffle? Prevents network from learning order of samples
        
        // Step 2: Create mini-batches
        auto batches = createBatches(X_shuffled, y_shuffled, batch_size);
        // Splits data into chunks of size batch_size
        // Example: 1000 samples, batch_size=32 â†’ 32 batches
        
        double total_loss = 0.0;
        
        // â”€â”€â”€ BATCH LOOP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for (const auto& batch : batches) {
            const Matrix& X_batch = batch.first;   // Input batch
            const Matrix& y_batch = batch.second;  // Target batch
            
            // Step 3: Forward pass
            Matrix predictions = forward(X_batch);
            // Run input through network to get predictions
            
            // Step 4: Compute loss
            double batch_loss = loss_function->calculate(predictions, y_batch);
            total_loss += batch_loss * X_batch.getRows();
            // Accumulate loss weighted by batch size
            
            // Step 5: Backward pass
            Matrix loss_grad = loss_function->gradient(predictions, y_batch);
            backward(loss_grad);
            // Compute gradients for all layers
            
            // Step 6: Update parameters
            updateParameters(learning_rate);
            // Apply gradient descent
        }
        
        // Step 7: Calculate average loss for epoch
        double avg_loss = total_loss / X_train.getRows();
        
        // Step 8: Print progress
        if (verbose && (epoch % 10 == 0 || epoch == epochs - 1)) {
            std::cout << "Epoch " << std::setw(4) << epoch + 1 << "/" << epochs 
                     << " - Loss: " << std::fixed << std::setprecision(6) << avg_loss 
                     << std::endl;
        }
    }
}

// Complete training cycle visualization:
//
// Epoch 1:
//   Shuffle â†’ Batch 1 â†’ Forward â†’ Loss â†’ Backward â†’ Update
//          â†’ Batch 2 â†’ Forward â†’ Loss â†’ Backward â†’ Update
//          â†’ ...
//          â†’ Batch 32 â†’ Forward â†’ Loss â†’ Backward â†’ Update
//   Print: "Epoch 1/100 - Loss: 0.523"
//
// Epoch 2:
//   Shuffle â†’ Batch 1 â†’ ...
//   ...
//
// Epoch 100:
//   Shuffle â†’ Batch 1 â†’ ...
//   Print: "Epoch 100/100 - Loss: 0.012"
```

---

## Complete Network Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  NEURAL NETWORK COMPLETE FLOW                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BUILDING PHASE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

NeuralNetwork network;

network.addLayer(new DenseLayer(784, 128, new ReLU()));
         â†“
    layers[0] = DenseLayer
         â”œâ”€ weights: (128Ã—784)
         â”œâ”€ biases: (128Ã—1)
         â””â”€ activation: ReLU

network.addLayer(new DenseLayer(128, 64, new ReLU()));
         â†“
    layers[1] = DenseLayer
         â”œâ”€ weights: (64Ã—128)
         â”œâ”€ biases: (64Ã—1)
         â””â”€ activation: ReLU

network.addLayer(new DenseLayer(64, 10, new Sigmoid()));
         â†“
    layers[2] = DenseLayer
         â”œâ”€ weights: (10Ã—64)
         â”œâ”€ biases: (10Ã—1)
         â””â”€ activation: Sigmoid

network.setLoss(new MSELoss());
         â†“
    loss_function = MSELoss

network.setOptimizer(new Adam(0.001));
         â†“
    optimizer = Adam
    use_optimizer = true


TRAINING PHASE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

network.train(X_train, y_train, epochs=100, batch_size=32, lr=0.01);

For each epoch:
  â”‚
  â”œâ”€ 1. Shuffle data
  â”‚    X_train, y_train â†’ randomize order
  â”‚
  â”œâ”€ 2. Create batches
  â”‚    1000 samples, batch_size=32 â†’ 32 batches
  â”‚
  â””â”€ 3. For each batch:
       â”‚
       â”œâ”€ a. Forward Pass
       â”‚    X_batch (32Ã—784)
       â”‚       â†“
       â”‚    layers[0]->forward(X)
       â”‚       â†“ ReLU(XÂ·Wâ‚€áµ€ + bâ‚€)
       â”‚    H1 (32Ã—128)
       â”‚       â†“
       â”‚    layers[1]->forward(H1)
       â”‚       â†“ ReLU(H1Â·Wâ‚áµ€ + bâ‚)
       â”‚    H2 (32Ã—64)
       â”‚       â†“
       â”‚    layers[2]->forward(H2)
       â”‚       â†“ Sigmoid(H2Â·Wâ‚‚áµ€ + bâ‚‚)
       â”‚    Å· (32Ã—10)
       â”‚
       â”œâ”€ b. Calculate Loss
       â”‚    loss = MSELoss->calculate(Å·, y_batch)
       â”‚         = (1/32) Ã— Î£(y - Å·)Â²
       â”‚
       â”œâ”€ c. Backward Pass
       â”‚    loss_grad = MSELoss->gradient(Å·, y_batch)
       â”‚              = -2(y - Å·) / 32
       â”‚       â†“
       â”‚    layers[2]->backward(loss_grad)
       â”‚       â†“ Compute âˆ‚L/âˆ‚Wâ‚‚, âˆ‚L/âˆ‚bâ‚‚, return âˆ‚L/âˆ‚H2
       â”‚    grad2 (32Ã—64)
       â”‚       â†“
       â”‚    layers[1]->backward(grad2)
       â”‚       â†“ Compute âˆ‚L/âˆ‚Wâ‚, âˆ‚L/âˆ‚bâ‚, return âˆ‚L/âˆ‚H1
       â”‚    grad1 (32Ã—128)
       â”‚       â†“
       â”‚    layers[0]->backward(grad1)
       â”‚       â†“ Compute âˆ‚L/âˆ‚Wâ‚€, âˆ‚L/âˆ‚bâ‚€, return âˆ‚L/âˆ‚X
       â”‚    grad0 (32Ã—784)
       â”‚
       â””â”€ d. Update Parameters
            For each layer:
              W_new = optimizer->update(W, âˆ‚L/âˆ‚W, "layer_i_weights")
              b_new = optimizer->update(b, âˆ‚L/âˆ‚b, "layer_i_biases")


PREDICTION PHASE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Matrix predictions = network.predict(X_test);

  X_test (10Ã—784)
     â†“
  forward(X_test)
     â†“
  layers[0]->forward â†’ (10Ã—128)
     â†“
  layers[1]->forward â†’ (10Ã—64)
     â†“
  layers[2]->forward â†’ (10Ã—10)
     â†“
  predictions (10Ã—10)


EVALUATION PHASE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

double test_loss = network.evaluate(X_test, y_test);
  = loss_function->calculate(network.forward(X_test), y_test)

double acc = network.accuracy(X_test, y_test);
  = (correct_predictions / total_samples) Ã— 100
```

---

## Complete Working Example

Now let me create a complete example program that shows everything working together:

```cpp
#include "nn/network.h"
#include "nn/layer.h"
#include "nn/activation.h"
#include "nn/loss.h"
#include "nn/optimizer.h"
#include <iostream>

int main() {
    std::cout << "=== Building Neural Network ===" << std::endl;
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // STEP 1: Create Network Object
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    NeuralNetwork network;
    // Empty container ready to hold layers
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // STEP 2: Add Layers (Build Architecture)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    // Input Layer â†’ Hidden Layer 1
    network.addLayer(new DenseLayer(2, 4, new ReLU()));
    // 2 inputs (xâ‚, xâ‚‚)
    // 4 hidden neurons
    // ReLU activation
    // Parameters: 2Ã—4 + 4 = 12
    
    // Hidden Layer 1 â†’ Output Layer
    network.addLayer(new DenseLayer(4, 1, new Sigmoid()));
    // 4 inputs (from previous layer)
    // 1 output (binary classification)
    // Sigmoid activation (outputs probability)
    // Parameters: 4Ã—1 + 1 = 5
    
    std::cout << "Network structure: 2 â†’ 4 â†’ 1" << std::endl;
    std::cout << "Total parameters: 12 + 5 = 17" << std::endl;
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // STEP 3: Set Loss Function
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    network.setLoss(new MSELoss());
    // Mean Squared Error for regression/binary classification
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // STEP 4: Set Optimizer (Optional)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    network.setOptimizer(new SGD(0.1, 0.9));
    // SGD with learning_rate=0.1, momentum=0.9
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // STEP 5: Prepare Training Data (XOR Problem)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Matrix X_train(4, 2);
    X_train.set(0, 0, 0); X_train.set(0, 1, 0);  // [0, 0]
    X_train.set(1, 0, 0); X_train.set(1, 1, 1);  // [0, 1]
    X_train.set(2, 0, 1); X_train.set(2, 1, 0);  // [1, 0]
    X_train.set(3, 0, 1); X_train.set(3, 1, 1);  // [1, 1]
    
    Matrix y_train(4, 1);
    y_train.set(0, 0, 0);  // 0 XOR 0 = 0
    y_train.set(1, 0, 1);  // 0 XOR 1 = 1
    y_train.set(2, 0, 1);  // 1 XOR 0 = 1
    y_train.set(3, 0, 0);  // 1 XOR 1 = 0
    
    std::cout << "\n=== Training Network ===" << std::endl;
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // STEP 6: Train the Network
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    network.train(
        X_train,        // Training inputs
        y_train,        // Training targets
        1000,           // epochs
        4,              // batch_size (use all 4 samples)
        0.1,            // learning_rate
        true            // verbose (print progress)
    );
    
    // What happens during training:
    // 
    // Epoch 1:
    //   Shuffle data
    //   Batch 1 (all 4 samples):
    //     Forward:  X â†’ Layer0 â†’ H1 â†’ Layer1 â†’ Å·
    //     Loss:     L = MSE(Å·, y) = high (random weights)
    //     Backward: âˆ‚L/âˆ‚Å· â†’ Layer1 â†’ Layer0
    //     Update:   W = W - lr Ã— âˆ‚L/âˆ‚W
    //   Print: "Epoch 1/1000 - Loss: 0.250000"
    //
    // Epoch 10:
    //   (same process)
    //   Print: "Epoch 10/1000 - Loss: 0.210000"
    //
    // ...
    //
    // Epoch 1000:
    //   (same process)
    //   Print: "Epoch 1000/1000 - Loss: 0.000800"
    
    std::cout << "\n=== Testing Network ===" << std::endl;
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // STEP 7: Make Predictions
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Matrix predictions = network.predict(X_train);
    
    std::cout << "\nPredictions:" << std::endl;
    for (size_t i = 0; i < 4; i++) {
        std::cout << "Input: [" << X_train.get(i,0) << ", " 
                  << X_train.get(i,1) << "] â†’ ";
        std::cout << "Predicted: " << std::fixed << std::setprecision(4) 
                  << predictions.get(i,0);
        std::cout << " | Target: " << y_train.get(i,0);
        
        if (std::abs(predictions.get(i,0) - y_train.get(i,0)) < 0.1) {
            std::cout << " âœ“" << std::endl;
        } else {
            std::cout << " âœ—" << std::endl;
        }
    }
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // STEP 8: Evaluate Performance
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    double test_loss = network.evaluate(X_train, y_train);
    std::cout << "\nTest Loss: " << test_loss << std::endl;
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // STEP 9: Display Network Summary
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    network.summary();
    
    // Output:
    // ========== Neural Network Summary ==========
    // Total Layers: 2
    // Optimizer: SGD
    // Loss Function: MSELoss
    //
    // --- Layer Details ---
    // Layer 1: DenseLayer (2 -> 4)
    // Layer 2: DenseLayer (4 -> 1)
    // ============================================
    
    return 0;
}
```

---

## Key Concepts Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NEURAL NETWORK CLASS: THE BIG PICTURE                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Network = Container
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Holds multiple layers (vector<Layer*>)
   â€¢ Holds loss function (Loss*)
   â€¢ Holds optimizer (Optimizer*)
   â€¢ Orchestrates training process

2. Layers = Building Blocks
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Each layer transforms input to output
   â€¢ Layers connected sequentially
   â€¢ Output of layer i = input of layer i+1

3. Forward Pass = Prediction
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Data flows INPUT â†’ Layer0 â†’ Layer1 â†’ ... â†’ OUTPUT
   â€¢ Each layer: output = activation(inputÂ·W + b)
   â€¢ Returns final prediction

4. Loss Function = Error Measure
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Compares prediction to target
   â€¢ Returns scalar: how wrong we are
   â€¢ Provides gradient for backprop

5. Backward Pass = Learning
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Gradients flow OUTPUT â†’ ... â†’ Layer1 â†’ Layer0 â†’ INPUT
   â€¢ Each layer computes âˆ‚L/âˆ‚W and âˆ‚L/âˆ‚b
   â€¢ Chain rule connects all gradients

6. Update Parameters = Improvement
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Use computed gradients to adjust weights
   â€¢ W_new = W_old - learning_rate Ã— âˆ‚L/âˆ‚W
   â€¢ Optimizer makes this smarter (momentum, adaptive rates)

7. Training Loop = Repeated Learning
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Repeat many times (epochs):
     1. Shuffle data
     2. Split into batches
     3. Forward â†’ Loss â†’ Backward â†’ Update
   â€¢ Loss decreases, accuracy increases

8. Prediction = Using Trained Network
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Forward pass without backward
   â€¢ No parameter updates
   â€¢ Just get predictions for new data
```

---

## Memory Management Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HOW NETWORK MANAGES MEMORY                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CREATION:
â•â•â•â•â•â•â•â•â•

NeuralNetwork network;
   â”‚
   â”œâ”€ layers (empty vector)
   â”œâ”€ loss_function (null)
   â””â”€ optimizer (null)


ADDING LAYERS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•

network.addLayer(new DenseLayer(2, 4, new ReLU()));
   â”‚
   â”‚ Create on heap:
   â”‚   DenseLayer* layer = new DenseLayer(2, 4, new ReLU());
   â”‚                          â”‚
   â”‚                          â”œâ”€ Allocates W (4Ã—2)
   â”‚                          â”œâ”€ Allocates b (4Ã—1)
   â”‚                          â””â”€ Stores ReLU* activation
   â”‚
   â””â”€ Network wraps in unique_ptr:
        layers.push_back(unique_ptr<Layer>(layer))
        
        Memory ownership transferred to network!
        Network will delete layer when destroyed


DESTRUCTION:
â•â•â•â•â•â•â•â•â•â•â•â•

} // network goes out of scope

network destructor called:
   â”‚
   â”œâ”€ layers vector destroyed
   â”‚    â”‚
   â”‚    â”œâ”€ layers[0].~unique_ptr()
   â”‚    â”‚    â””â”€ delete DenseLayer
   â”‚    â”‚         â””â”€ DenseLayer destructor
   â”‚    â”‚              â”œâ”€ delete activation (ReLU)
   â”‚    â”‚              â”œâ”€ free W memory
   â”‚    â”‚              â””â”€ free b memory
   â”‚    â”‚
   â”‚    â”œâ”€ layers[1].~unique_ptr()
   â”‚    â”‚    â””â”€ (same process)
   â”‚    â”‚
   â”‚    â””â”€ ...
   â”‚
   â”œâ”€ loss_function.~unique_ptr()
   â”‚    â””â”€ delete MSELoss
   â”‚
   â””â”€ optimizer.~unique_ptr()
        â””â”€ delete Adam

All memory automatically freed!
No memory leaks!
```

---

This comprehensive guide shows you how the `NeuralNetwork` class ties everything together - layers, activations, loss, and training - into a complete machine learning system!
